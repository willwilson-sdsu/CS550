{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Organizing Maps\n",
    "We will use the Iris data set for both the SOM and ANN examples \n",
    "I am using it because it is a fairly clean dataset that is readily available\n",
    "SOMs and back propigation nueral networks are not the best fits for this data set \n",
    "Neural networks are good for data that is not cleanly linearly seperable. That is not the cases with Iris.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All my imports\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manual example I found at \n",
    "# Source: https://github.com/JRC1995/Self-Organizing-Map/blob/master/SOM.ipynb\n",
    "### I dont really like how they defined error\n",
    "### If you go through the code, it is really just how much it changes between epochs (iterations)\n",
    "## I will also show using  scikit learn\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "patterns = iris.data\n",
    "classes = iris.target\n",
    "\n",
    "sample_no = np.random.randint(0,len(patterns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A heuristic formula for calculating no. of map units\n",
    "#source: https://stackoverflow.com/questions/19163214/kohonen-self-organizing-maps-determining-the-number-of-neurons-and-grid-size\n",
    "\n",
    "def mapunits(input_len,size='small'):\n",
    "    \n",
    "    heuristic_map_units = 5*input_len**0.54321\n",
    "     \n",
    "    if size == 'big':\n",
    "        heuristic_map_units = 4*(heuristic_map_units)\n",
    "    else:\n",
    "        heuristic_map_units = 0.25*(heuristic_map_units)\n",
    "        \n",
    "    return heuristic_map_units\n",
    "        \n",
    "        \n",
    "map_units = mapunits(len(patterns),size='big')\n",
    "print(\"Heuristically computed appropriate no. of map units: \"+str(int(map_units)))\n",
    "\n",
    "#For reference purpose only - however this function can be used to automatically calculate the SOM dimensions\n",
    "#from data length. I will still be specifying the SOM dimensions manually, anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Eucli_dists(MAP,x):\n",
    "    x = x.reshape((1,1,-1))\n",
    "    #print(x)\n",
    "    Eucli_MAP = MAP - x\n",
    "    Eucli_MAP = Eucli_MAP**2\n",
    "    Eucli_MAP = np.sqrt(np.sum(Eucli_MAP,2))\n",
    "    return Eucli_MAP\n",
    "\n",
    "input_dimensions = 4\n",
    "\n",
    "map_width = 9\n",
    "map_height = 5\n",
    "MAP = np.random.uniform(size=(map_height,map_width,input_dimensions))\n",
    "prev_MAP = np.zeros((map_height,map_width,input_dimensions))\n",
    "\n",
    "radius0 = max(map_width,map_height)/2\n",
    "learning_rate0 = 0.1\n",
    "\n",
    "coordinate_map = np.zeros([map_height,map_width,2],dtype=np.int32)\n",
    "\n",
    "for i in range(0,map_height):\n",
    "    for j in range(0,map_width):\n",
    "        coordinate_map[i][j] = [i,j]\n",
    "\n",
    "epochs = 500\n",
    "radius=radius0\n",
    "learning_rate = learning_rate0\n",
    "max_iterations = len(patterns)+1\n",
    "too_many_iterations = 10*max_iterations\n",
    "\n",
    "convergence = [1]\n",
    "\n",
    "timestep=1\n",
    "e=0.001 \n",
    "flag=0\n",
    "\n",
    "epoch=0\n",
    "while epoch<epochs:\n",
    "    \n",
    "    shuffle = np.random.randint(len(patterns), size=len(patterns))\n",
    "    for i in range(len(patterns)):\n",
    "        \n",
    "        # difference between prev_MAP and MAP\n",
    "        J = np.linalg.norm(MAP - prev_MAP)\n",
    "        #print(J)\n",
    "        # J = || euclidean distance between previous MAP and current MAP  ||\n",
    "\n",
    "        if  J <= e: #if converged (convergence criteria)\n",
    "            flag=1\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #if timestep == max_iterations and timestep != too_many_iterations:\n",
    "            #    epochs += 1\n",
    "            #    max_iterations = epochs*len(patterns)\n",
    "            \n",
    "            pattern = patterns[shuffle[i]]\n",
    "            pattern_ary = np.tile(pattern, (map_height, map_width, 1))\n",
    "            Eucli_MAP = np.linalg.norm(pattern_ary - MAP, axis=2)\n",
    "            \n",
    "            # Get the best matching unit(BMU) which is the one with the smallest Euclidean distance\n",
    "            BMU = np.unravel_index(np.argmin(Eucli_MAP, axis=None), Eucli_MAP.shape)\n",
    "            #BMU[1] = np.argmin(Eucli_MAP, 1)[int(BMU[0])]\n",
    "    \n",
    "            #Eucli_from_BMU = Eucli_dists(coordinate_map,BMU)  \n",
    "        \n",
    "            prev_MAP = np.copy(MAP)\n",
    "             \n",
    "            for i in range(map_height):\n",
    "                for j in range(map_width):\n",
    "                    distance = np.linalg.norm([i - BMU[0], j - BMU[1]])\n",
    "                    if distance <= radius:\n",
    "                        #theta = math.exp(-(distance**2)/(2*(radius**2)))\n",
    "                        MAP[i][j] = MAP[i][j] + learning_rate*(pattern-MAP[i][j])\n",
    "            \n",
    "            learning_rate = learning_rate0*(1-(epoch/epochs))\n",
    "            #time_constant = max_iterations/math.log(radius) \n",
    "            radius = radius0*math.exp(-epoch/epochs)\n",
    "            #print([learning_rate, radius])\n",
    "            \n",
    "            timestep+=1\n",
    "    \n",
    "    if J < min(convergence):\n",
    "        print('Lower error found: %s' %str(J) + ' at epoch: %s' % str(epoch))\n",
    "        print('\\tLearning rate: ' + str(learning_rate))\n",
    "        print('\\tNeighbourhood radius: ' + str(radius))\n",
    "        MAP_final = MAP\n",
    "    convergence.append(J)\n",
    "    \n",
    "    if flag==1:\n",
    "        break\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a plot of the error at each epoch to show convergence, but this is guaranteed in SOM\n",
    "# due to the learning rate and neighbourhood decay\n",
    "plt.plot(convergence)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "print('Number of timesteps: ' + str(timestep))\n",
    "print('Final error: ' + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.misc import toimage\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "BMU = np.zeros([2],dtype=np.int32)\n",
    "result_map = np.zeros([map_height,map_width,3],dtype=np.float32)\n",
    "\n",
    "i=0\n",
    "for pattern in patterns:\n",
    "    \n",
    "    pattern_ary = np.tile(pattern, (map_height, map_width, 1))\n",
    "    Eucli_MAP = np.linalg.norm(pattern_ary - MAP_final, axis=2)\n",
    "\n",
    "    # Get the best matching unit(BMU) which is the one with the smallest Euclidean distance\n",
    "    BMU = np.unravel_index(np.argmin(Eucli_MAP, axis=None), Eucli_MAP.shape)\n",
    "    \n",
    "    x = BMU[0]\n",
    "    y = BMU[1]\n",
    "    \n",
    "    #if classes[i] == 'Iris-setosa':\n",
    "    if classes[i] == 0:\n",
    "        if result_map[x][y][0] <= 0.5:\n",
    "            result_map[x][y] += np.asarray([0.5,0,0])\n",
    "    elif classes[i] == 1:\n",
    "        #classes[i] == 'Iris-virginica':\n",
    "        if result_map[x][y][1] <= 0.5:\n",
    "            result_map[x][y] += np.asarray([0,0.5,0])\n",
    "    elif classes[i] == 2:\n",
    "        #classes[i] == 'Iris-versicolor':\n",
    "        if result_map[x][y][2] <= 0.5:\n",
    "            result_map[x][y] += np.asarray([0,0,0.5])\n",
    "    i+=1\n",
    "result_map = np.flip(result_map,0)\n",
    "    \n",
    "#print result_map\n",
    "\n",
    "print(\"Red = Iris-Setosa\")\n",
    "print(\"Blue = Iris-Virginica\")\n",
    "print(\"Green = Iris-Versicolor\")\n",
    "\n",
    "plt.imshow(result_map, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do it again using scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_som.som import SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Iris data in case I dont do the previous part first\n",
    "iris = datasets.load_iris()\n",
    "iris_data = iris.data[:,]\n",
    "iris_label = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SOM\n",
    "iris_som = SOM(m=3, n=1, dim=4)\n",
    "iris_som.fit(iris_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = iris_som.predict(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(5,7))\n",
    "# We are just using two dimensions\n",
    "# In this case x and y are the Sepal length vs width\n",
    "# This is a visualization of legth vs width and not of the SOM map\n",
    "x = iris_data[:,0]\n",
    "y = iris_data[:,1]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "ax[0].scatter(x, y, c=iris_label, cmap=ListedColormap(colors))\n",
    "ax[0].title.set_text('Actual Classes')\n",
    "ax[1].scatter(x, y, c=predictions, cmap=ListedColormap(colors))\n",
    "ax[1].title.set_text('SOM Predictions')\n",
    "#plt.savefig('iris_example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to do it again with max_iter = 1 so you can see how it starts\n",
    "# Fit the SOM\n",
    "# We can keep running this to see how it initalizes\n",
    "## The Iris data set does not give the most interesting results\n",
    "\n",
    "iris_som = SOM(m=3, n=1, dim=4, max_iter=1)\n",
    "iris_som.fit(iris_data)\n",
    "predictions = iris_som.predict(iris_data)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,7))\n",
    "x = iris_data[:,0]\n",
    "y = iris_data[:,1]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "ax.scatter(x, y, c=predictions, cmap=ListedColormap(colors))\n",
    "ax.title.set_text('SOM Predictions')\n",
    "#plt.savefig('iris_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "# Get digits dataset\n",
    "# Might not be best choice because it has 64 features.\n",
    "#https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# I had to crank max_iter up on this and play with hidden layers to get it to converge\n",
    "# Changed solver from the default\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(3, 6), random_state=1,max_iter = 3000)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"F1 Score\",metrics.f1_score(y_test,predictions,average=None))\n",
    "metrics.precision_score(y_test,predictions,average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf, X_test, y_test)  \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
