{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "The intent of this section is to show the results of k-means clustering\n",
    "We will use the Iris data set. I will color the type of Iris for demonstration purposes, but remember that K-means is unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All my imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from fcmeans import FCM\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair plots can help us in our data exploration phase\n",
    "We can visualize how clusters might look \n",
    "When the pair plot is with itself, it shows a distribution on that variable.\n",
    "Look at  Sepal Width (assuming I properly mapped the columns). \n",
    "Does it add much value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some pair plots of the features\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = [\"Sepal Length\",\"Sepal Width\",\"Pedal Length\",\"Pedal Width\"]\n",
    "df[\"Species\"] = y\n",
    "sns.pairplot(df,hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pair plots we can get an idea on which variables contribute the most. We could likely removed Sepa Width. If we wanted to keep two original varialbes, the pedal variables seem to provide the most information.\n",
    "However, we want to do a demo of PCA ... so lets look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Code - Down to 3 dimensions at first, just for example\n",
    "X2 = X.copy()\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X2)\n",
    "X2 = pca.transform(X2)\n",
    "df2 = pd.DataFrame(X2)\n",
    "df2.columns = [\"Component1\",\"Component2\",\"Component3\"]\n",
    "df2[\"Species\"] = y\n",
    "print(df2.describe())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Warning about deprecated methods were annoying me\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X2[y == label, 0].mean(),\n",
    "              X2[y == label, 1].mean() + 1.5,\n",
    "              X2[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(float)\n",
    "ax.scatter(X2[:, 0], X2[:, 1], X2[:, 2], c=y, cmap=plt.cm.nipy_spectral,\n",
    "           edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are down to 3 dimensions, we can visualize it better than 4 dimensions\n",
    "We can see a nice separation for Setosa, but the other two overlap a bit\n",
    "\n",
    "We will set n_init to 1 for one test. \n",
    "The n_init variable controls how many times it will initialize with different centroids\n",
    "The implemention of kmeans in scikitlearn will run the algorithm with n_init different starts and pick the model that it thinks is best based on an inertia meteric.\n",
    "Inertia is calculated uses sum of squares to closest cluster center.\n",
    "It is possible that the first choice is the best choice and n_init = 1 could have good results. The default is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n",
    "\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_3', KMeans(n_clusters=3)),\n",
    "              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,\n",
    "                                               init='random'))]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 2, 2], elev=48, azim=134)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X2[:, 0], X2[:, 1], X2[:, 2],\n",
    "               c=labels.astype(float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, 2, 2], elev=48, azim=134)\n",
    "\n",
    "for name, label in [('Setosa', 1),\n",
    "                    ('Versicolour', 2),\n",
    "                    ('Virginica', 0)]:\n",
    "    ax.text3D(X2[y == label, 0].mean(),\n",
    "              X2[y == label, 1].mean(),\n",
    "              X2[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "#y = np.choose(y, [1, 2, 0]).astype(int)\n",
    "ax.scatter(X2[:, 0], X2[:, 1], X2[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving to 2D because it is easier for me to visualize\n",
    "# and that is the code I am reusing\n",
    "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "\n",
    "# I am pulling the data back in because I copied the code mostly verbatim\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do it again, but use pedal features\n",
    "# We could also use PCA, but then we lose understanding of the relationship between variables\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "    plt.xlabel(iris.feature_names[2])\n",
    "    plt.ylabel(iris.feature_names[3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont know about you, but I think this looks a lot better when we use pedal variables\n",
    "I am curious what it will look like if I use PCA to 2 variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are similar to using just the petal features, but not quite.\n",
    "We also see that the domain of the components are not the same as the original variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "I am going to start with the reduced dimensions from the previous block\n",
    "This dataset is clean enough that I don't expect to need to use anything other than the default settings.\n",
    "I didn't cover the predict method for k-nearest neighbor but they all work about the same\n",
    "\n",
    "The scikitlearn documentation page uses Sepal measurements. My guess is that they did it because it makes a more interesting example \n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too easy\n",
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Lets test a prediction\n",
    "print(\"The prediction is\",iris.target_names[clf.predict([[2., 2.]])][0])\n",
    "# We could look at other things such as predict_proba if we want to see other classes\n",
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X, y)\n",
    "probs = clf.predict_proba([[2., 2.]])\n",
    "print(\"The probability array is\",probs)\n",
    "# Now we can see the estimated probabilty of each class. \n",
    "# Maybe I should clean up the results\n",
    "print(\"Probability of {} is {:0.2f}%\".format(iris.target_names[0],probs[0][0]*100))\n",
    "print(\"Probability of {} is {:0.2f}%\".format(iris.target_names[1],probs[0][1]*100))\n",
    "print(\"Probability of {} is {:0.2f}%\".format(iris.target_names[2],probs[0][2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "# https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8\n",
    "# BTW, this is a good article. \n",
    "# It is much better than the one I used when I was writing the slides\n",
    "# ... but I just found it while working on the demo notebook\n",
    "\n",
    "\n",
    "feature_names = [\"Component 1\",\"Component 2\"]\n",
    "classes = iris.target_names\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "# The classification SVC model\n",
    "model = svm.SVC(kernel=\"linear\")\n",
    "clf = model.fit(X, y)\n",
    "fig, ax = plt.subplots()\n",
    "# title for the plots\n",
    "title = ('Decision surface of linear SVC')\n",
    "# Set-up grid for plotting.\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "ax.set_ylabel(\"{}\".format(feature_names[0]))\n",
    "ax.set_xlabel(\"{}\".format(feature_names[1]))\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D example\n",
    "X = iris.data\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "Y = iris.target\n",
    "\n",
    "#make it binary classification problem\n",
    "X = X[np.logical_or(Y==0,Y==1)]\n",
    "Y = Y[np.logical_or(Y==0,Y==1)]\n",
    "model = svm.SVC(kernel='linear')\n",
    "clf = model.fit(X, Y)\n",
    "# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n",
    "# Solve for w3 (z)\n",
    "z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]\n",
    "tmp = np.linspace(-1,1,2)\n",
    "x,y = np.meshgrid(tmp,tmp)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')\n",
    "ax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')\n",
    "ax.plot_surface(x, y, z(x,y))\n",
    "ax.view_init(15,45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new example with random points\n",
    "# Show the margin\n",
    "# Show the support vectors\n",
    "\n",
    "# Set seed so we always get the same results.\n",
    "# Maybe change and run again\n",
    "np.random.seed(2)\n",
    "\n",
    "# we create 40 linearly separable points\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "Y = [0] * 20 + [1] * 20\n",
    "# fit the model\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# get the separating hyperplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "# Get a line for x \n",
    "xx = np.linspace(-5, 5)\n",
    "# Build the line for yy using xx and the model\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# Calculate the margine using coefficient in the model\n",
    "## Sq Root of Sum of squares\n",
    "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "# Offset the separator line by the margin\n",
    "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.plot(xx, yy, \"k-\")\n",
    "plt.plot(xx, yy_down, \"k-\")\n",
    "plt.plot(xx, yy_up, \"k-\")\n",
    "# The first scatter plot grabs just the points on the line and makes them stand out\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    " facecolors=\"none\", zorder=10, edgecolors=\"k\")\n",
    "\n",
    "# Plot all the points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n",
    " edgecolors=\"k\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
